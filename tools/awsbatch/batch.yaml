---
# Create all the infrastructure for AWS Batch with RIOS
# This includes its own VPC, subnets and security group.
# These subnets are spread accross the first 3 availability zones
# for the Region.
# Private and public subnets are created with a NAT in the first
# public subnet so the private subnets have internet access 
# which is a requirement for Batch.
# An endpoint is created for S3 so S3 file access does not go
# through the NAT.
# Also created is an ECR for saving the docker image that is used for processing,
#
# Use the script createbatch.py for stack creation and modification.
# deletebatch.py for deletion.
AWSTemplateFormatVersion: '2010-09-09'
Description: 'RIOS with AWS Batch using CloudFormation'
# These values can be altered here or overridden with the params
# for createbatch.py.
Parameters:
  ServiceName:
    Type: String
    Default: rios
  VCPUS:
    Type: Number
    Default: 1
  MaxMemory:
    Type: Number
    Default: 4000
  MaxVCPUS:
    Type: Number
    Default: 128
  InstanceType:
    Type: String
    Default: optimal  # for x86. Use m7g etc for Gravitron
  MainVolumeSize:
    Type: Number
    Default: 32  # increase if you run out of space with 'main' jobs
  
Resources:
  # Create our own vpc for resources so we are separate
  # from whatever else the running account has.
  # And we can easily determine the output names of resources from
  # CloudFormation
  BatchVPC:
    Type: AWS::EC2::VPC
    Properties:
      CidrBlock: 10.0.0.0/16
      # Below needed for Batch it seems
      EnableDnsSupport: 'true'
      EnableDnsHostnames: 'true'
  # Create a public subnet for each availability zone
  BatchPublicSubnet1:
    Type: AWS::EC2::Subnet
    Properties:
      CidrBlock: 10.0.0.0/24
      VpcId:
        Ref: BatchVPC
      AvailabilityZone: !Select 
        - 0
        - Fn::GetAZs: !Ref 'AWS::Region'
  BatchPublicSubnet2:
    Type: AWS::EC2::Subnet
    Properties:
      CidrBlock: 10.0.1.0/24
      VpcId:
        Ref: BatchVPC
      AvailabilityZone: !Select 
        - 1
        - Fn::GetAZs: !Ref 'AWS::Region'
  BatchPublicSubnet3:
    Type: AWS::EC2::Subnet
    Properties:
      CidrBlock: 10.0.2.0/24
      VpcId:
        Ref: BatchVPC
      AvailabilityZone: !Select 
        - 2
        - Fn::GetAZs: !Ref 'AWS::Region'
  # Create a private subnet for each availability zone
  BatchPrivateSubnet1:
    Type: AWS::EC2::Subnet
    Properties:
      CidrBlock: 10.0.10.0/24
      VpcId:
        Ref: BatchVPC
      AvailabilityZone: !Select 
        - 0
        - Fn::GetAZs: !Ref 'AWS::Region'
  BatchPrivateSubnet2:
    Type: AWS::EC2::Subnet
    Properties:
      CidrBlock: 10.0.11.0/24
      VpcId:
        Ref: BatchVPC
      AvailabilityZone: !Select 
        - 1
        - Fn::GetAZs: !Ref 'AWS::Region'
  BatchPrivateSubnet3:
    Type: AWS::EC2::Subnet
    Properties:
      CidrBlock: 10.0.12.0/24
      VpcId:
        Ref: BatchVPC
      AvailabilityZone: !Select 
        - 2
        - Fn::GetAZs: !Ref 'AWS::Region'

  # A security group for everything to run as
  BatchSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupName: !Sub '${ServiceName} Security Group'
      GroupDescription: "Security Group for RIOS VPC"
      VpcId: !Ref BatchVPC

  # Allow traffic between the nodes running the jobs
  BatchIngress:
    Type: AWS::EC2::SecurityGroupIngress
    DependsOn: BatchSecurityGroup
    Properties:
      GroupId: !Ref BatchSecurityGroup
      IpProtocol: tcp
      FromPort: 30000
      ToPort: 50000
      SourceSecurityGroupId: !Ref BatchSecurityGroup

  # Jobs must have internet connectivity otherwise they won't run...
  # They need to talk to ECS/CloudWatch. 
  # Create an endpoint for S3 so traffic doesn't need to go through 
  # the internet gateway (and cost)
  PublicRouteTable1:
    Type: "AWS::EC2::RouteTable"
    Properties:
      VpcId: !Ref BatchVPC
  PublicRouteTable2:
    Type: "AWS::EC2::RouteTable"
    Properties:
      VpcId: !Ref BatchVPC
  PublicRouteTable3:
    Type: "AWS::EC2::RouteTable"
    Properties:
      VpcId: !Ref BatchVPC
  InternetGateway:
    Type: "AWS::EC2::InternetGateway"
  VPCGatewayAttachment:
    Type: "AWS::EC2::VPCGatewayAttachment"
    Properties:
      VpcId: !Ref BatchVPC
      InternetGatewayId: !Ref InternetGateway

  RoutePublicGateway1:
   DependsOn: InternetGateway
   Type: AWS::EC2::Route
   Properties:
      RouteTableId: !Ref PublicRouteTable1
      DestinationCidrBlock: '0.0.0.0/0'
      GatewayId: !Ref InternetGateway
  RoutePublicGateway2:
   DependsOn: InternetGateway
   Type: AWS::EC2::Route
   Properties:
      RouteTableId: !Ref PublicRouteTable2
      DestinationCidrBlock: '0.0.0.0/0'
      GatewayId: !Ref InternetGateway
  RoutePublicGateway3:
   DependsOn: InternetGateway
   Type: AWS::EC2::Route
   Properties:
      RouteTableId: !Ref PublicRouteTable3
      DestinationCidrBlock: '0.0.0.0/0'
      GatewayId: !Ref InternetGateway

  # Associate the public route table with each subnet
  PublicSubnet1RouteTableAssociation:
    Type: "AWS::EC2::SubnetRouteTableAssociation"
    Properties:
      RouteTableId: !Ref PublicRouteTable1
      SubnetId: !Ref BatchPublicSubnet1
  PublicSubnet2RouteTableAssociation:
    Type: "AWS::EC2::SubnetRouteTableAssociation"
    Properties:
      RouteTableId: !Ref PublicRouteTable2
      SubnetId: !Ref BatchPublicSubnet2
  PublicSubnet3RouteTableAssociation:
    Type: "AWS::EC2::SubnetRouteTableAssociation"
    Properties:
      RouteTableId: !Ref PublicRouteTable3
      SubnetId: !Ref BatchPublicSubnet3

  # NAT each private subnet
  PrivateRouteTable1:
    Type: "AWS::EC2::RouteTable"
    Properties:
      VpcId: !Ref BatchVPC
  PrivateRouteTable2:
    Type: "AWS::EC2::RouteTable"
    Properties:
      VpcId: !Ref BatchVPC
  PrivateRouteTable3:
    Type: "AWS::EC2::RouteTable"
    Properties:
      VpcId: !Ref BatchVPC

  # NAT gateway in first AZ      
  NATGatewayEIP1:
   DependsOn: BatchVPC
   Type: AWS::EC2::EIP
   Properties:
      Domain: vpc
  NATGateway1:
   Type: AWS::EC2::NatGateway
   Properties:
      AllocationId: !GetAtt NATGatewayEIP1.AllocationId
      SubnetId: !Ref BatchPublicSubnet1

  RouteNATGateway1:
   DependsOn: NATGateway1
   Type: AWS::EC2::Route
   Properties:
      RouteTableId: !Ref PrivateRouteTable1
      DestinationCidrBlock: '0.0.0.0/0'
      NatGatewayId: !Ref NATGateway1
  RouteNATGateway2:
   DependsOn: NATGateway1
   Type: AWS::EC2::Route
   Properties:
      RouteTableId: !Ref PrivateRouteTable2
      DestinationCidrBlock: '0.0.0.0/0'
      NatGatewayId: !Ref NATGateway1
  RouteNATGateway3:
   DependsOn: NATGateway1
   Type: AWS::EC2::Route
   Properties:
      RouteTableId: !Ref PrivateRouteTable3
      DestinationCidrBlock: '0.0.0.0/0'
      NatGatewayId: !Ref NATGateway1

  # Associate the private route table with each subnet
  PrivateSubnet1RouteTableAssociation:
    Type: "AWS::EC2::SubnetRouteTableAssociation"
    Properties:
      RouteTableId: !Ref PrivateRouteTable1
      SubnetId: !Ref BatchPrivateSubnet1
  PrivateSubnet2RouteTableAssociation:
    Type: "AWS::EC2::SubnetRouteTableAssociation"
    Properties:
      RouteTableId: !Ref PrivateRouteTable2
      SubnetId: !Ref BatchPrivateSubnet2
  PrivateSubnet3RouteTableAssociation:
    Type: "AWS::EC2::SubnetRouteTableAssociation"
    Properties:
      RouteTableId: !Ref PrivateRouteTable3
      SubnetId: !Ref BatchPrivateSubnet3


  # Allow S3 traffic to go through an internet gateway
  S3GatewayEndpoint:
    Type: AWS::EC2::VPCEndpoint
    Properties:
      VpcEndpointType: 'Gateway'
      VpcId: !Ref BatchVPC
      ServiceName: !Sub 'com.amazonaws.${AWS::Region}.s3'
      PolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal: '*'
            Action: 
               - 's3:*Object'
               - 's3:ListBucket'
            Resource: 
               - 'arn:aws:s3:::*/*'
               - 'arn:aws:s3:::*'
      RouteTableIds:
        - !Ref PrivateRouteTable1
        - !Ref PrivateRouteTable2
        - !Ref PrivateRouteTable3

  # Create an ECR to hold the image that contains RIOS
  # (and any other packages the function needs)
  # Expires older untagged images
  BatchRepository:
    Type: AWS::ECR::Repository
    Properties:
      RepositoryName: !Sub '${ServiceName}ecr'
      LifecyclePolicy:
        LifecyclePolicyText: |
          {
            "rules": [
                {
                    "rulePriority": 1,
                    "description": "Expire images older than 1 day",
                    "selection": {
                        "tagStatus": "untagged",
                        "countType": "sinceImagePushed",
                        "countUnit": "days",
                        "countNumber": 1
                    },
                    "action": {
                        "type": "expire"
                    }
                }
            ]
          }

  # another repo that the user can copy 'main' docker images with their main scripts
  BatchRepositoryMain:
    Type: AWS::ECR::Repository
    Properties:
      RepositoryName: !Sub '${ServiceName}ecrmain'
      LifecyclePolicy:
        LifecyclePolicyText: |
          {
            "rules": [
                {
                    "rulePriority": 1,
                    "description": "Expire images older than 1 day",
                    "selection": {
                        "tagStatus": "untagged",
                        "countType": "sinceImagePushed",
                        "countUnit": "days",
                        "countNumber": 1
                    },
                    "action": {
                        "type": "expire"
                    }
                }
            ]
          }
       
  # Allow jobs to submit other jobs. Handy in case the user
  # runs their own 'main' script in the queue and wants to 
  # fire off sub jobs
  SubmitJobsManagedPolicy:
    Type: AWS::IAM::ManagedPolicy
    Properties:
      Description: Policy for allowing jobs to submit other jobs
      Path: /
      PolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Action: 
            - 'batch:SubmitJob'
          Resource:
            # Sorry can't nail down to particular queue and defn
            # as this creates a circular dependency
            - !Sub 'arn:aws:batch:${AWS::Region}:${AWS::AccountId}:job-queue/*'
            - !Sub 'arn:aws:batch:${AWS::Region}:${AWS::AccountId}:job-definition/*'
      
  IamInstanceProfile:
    Type: AWS::IAM::InstanceProfile
    Properties:
      Roles:
      - Ref: EcsInstanceRole
  # This is the user that the batch workers run as.
  # Ensure we attach all the permissions it will need.
  EcsInstanceRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2008-10-17'
        Statement:
        - Sid: ''
          Effect: Allow
          Principal:
            Service: ec2.amazonaws.com
          Action: sts:AssumeRole
      ManagedPolicyArns:
      - arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceforEC2Role
      - arn:aws:iam::aws:policy/CloudWatchFullAccess
      - arn:aws:iam::aws:policy/AWSCloudFormationReadOnlyAccess
      - arn:aws:iam::aws:policy/AmazonS3FullAccess
      - !Ref SubmitJobsManagedPolicy
  # The worker job. Set the S3 bucket and SQS queues info in the 
  # enironment.
  BatchProcessingJobDefinition:
    Type: AWS::Batch::JobDefinition
    Properties:
      Type: container
      JobDefinitionName: !Sub '${ServiceName}JobDefinition'
      ContainerProperties:
        Image: !Join ['', [!GetAtt BatchRepository.RepositoryUri, ":latest"]]
        Vcpus: !Ref VCPUS
        Memory: !Ref MaxMemory

  # job definition for the 'main' script
  BatchProcessingJobDefinitionMain:
    Type: AWS::Batch::JobDefinition
    Properties:
      Type: container
      JobDefinitionName: !Sub '${ServiceName}JobDefinitionMain'
      ContainerProperties:
        Image: !Join ['', [!GetAtt BatchRepositoryMain.RepositoryUri, ":latest"]]
        Vcpus: !Ref VCPUS
        Memory: !Ref MaxMemory
        Environment:
          - Name: "RIOS_AWSBATCH_STACK"
            Value: !Ref "AWS::StackName"
          - Name: "RIOS_AWSBATCH_REGION"
            Value: !Ref "AWS::Region"

  # Our queues - a different one for 'main' jobs
  # as they usually need a different compute environment
  # with more storage.
  BatchProcessingJobQueueWorker:
    Type: AWS::Batch::JobQueue
    Properties:
      JobQueueName: !Sub '${ServiceName}JobQueueWorker'
      Priority: 1
      ComputeEnvironmentOrder:
      - Order: 1
        ComputeEnvironment:
          Ref: ComputeEnvironmentWorker
  BatchProcessingJobQueueMain:
    Type: AWS::Batch::JobQueue
    Properties:
      JobQueueName: !Sub '${ServiceName}JobQueueMain'
      Priority: 1
      ComputeEnvironmentOrder:
      - Order: 1
        ComputeEnvironment:
          Ref: ComputeEnvironmentMain
  # Compute Environment - set subnets and security group etc.
  ComputeEnvironmentWorker:
    Type: AWS::Batch::ComputeEnvironment
    Properties:
      Type: MANAGED
      ComputeResources:
        Type: EC2
        MinvCpus: 0
        MaxvCpus: !Ref MaxVCPUS
        InstanceTypes:
        - !Ref InstanceType
        Subnets:
        - !Ref BatchPrivateSubnet1
        - !Ref BatchPrivateSubnet2
        - !Ref BatchPrivateSubnet3
        SecurityGroupIds:
        - !Ref BatchSecurityGroup
        InstanceRole:
          Ref: IamInstanceProfile
  # Same as ComputeEnvironmentWorker but has a potentially larger
  # storage for saving large outputs - see the launch template
  ComputeEnvironmentMain:
    Type: AWS::Batch::ComputeEnvironment
    Properties:
      Type: MANAGED
      ComputeResources:
        Type: EC2
        MinvCpus: 0
        MaxvCpus: !Ref MaxVCPUS
        InstanceTypes:
        - !Ref InstanceType
        Subnets:
        - !Ref BatchPrivateSubnet1
        - !Ref BatchPrivateSubnet2
        - !Ref BatchPrivateSubnet3
        SecurityGroupIds:
        - !Ref BatchSecurityGroup
        InstanceRole:
          Ref: IamInstanceProfile
        LaunchTemplate:
          LaunchTemplateId: !Ref LaunchTemplate
          Version: !GetAtt LaunchTemplate.LatestVersionNumber
  # Launch template - increase default storage available
  # https://repost.aws/knowledge-center/batch-job-failure-disk-space
  # https://docs.aws.amazon.com/batch/latest/userguide/launch-templates.html
  LaunchTemplate:
    Type: AWS::EC2::LaunchTemplate
    Properties:
      LaunchTemplateData:
         BlockDeviceMappings:
           - DeviceName: /dev/xvda
             Ebs:
               VolumeType: gp2
               VolumeSize: !Ref MainVolumeSize
               DeleteOnTermination: true
     
# Outputs that the main script can queury to find
# the names and paths of things.
Outputs:
  VPC:
    Value:
      Ref: BatchVPC
  ComputeEnvironmentArn:
    Value:
      Ref: ComputeEnvironmentWorker
  ComputeEnvironmentMainArn:
    Value:
      Ref: ComputeEnvironmentMain
  BatchProcessingJobQueueArn:
    Value:
      Ref: BatchProcessingJobQueueWorker
  BatchProcessingJobQueueMainArn:
    Value:
      Ref: BatchProcessingJobQueueMain
  BatchProcessingJobQueueName:
    Value: !Sub '${ServiceName}JobQueueWorker'
  BatchProcessingJobQueueNameMain:
    Value: !Sub '${ServiceName}JobQueueMain'
  BatchProcessingJobDefinitionArn:
    Value:
      Ref: BatchProcessingJobDefinition
  BatchProcessingJobDefinitionMainArn:
    Value:
      Ref: BatchProcessingJobDefinitionMain
  BatchProcessingJobDefinitionName:
    Value: !Sub '${ServiceName}JobDefinition'
  BatchProcessingJobDefinitionMainName:
    Value: !Sub '${ServiceName}JobDefinitionMain'
  BatchECR:
    Value: !GetAtt BatchRepository.RepositoryUri
  BatchECRMain:
    Value: !GetAtt BatchRepositoryMain.RepositoryUri
  BatchVCPUS:
    Value: !Ref VCPUS
  BatchMaxVCPUS:
    Value: !Ref MaxVCPUS
