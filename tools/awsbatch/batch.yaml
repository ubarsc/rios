---
# Create all the infrastructure for AWS Batch with RIOS
# This includes its own VPC, subnets and security group.
# These subnets are spread accross the first 3 availibility zones
# For the Region.
# Also created is an ECR for saving the docker image that is used for processing,
#
# Use the script createbatch.py for stack creation and modification.
# deletebatch.py for deletion.
AWSTemplateFormatVersion: '2010-09-09'
Description: 'RIOS with AWS Batch using CloudFormation'
# These values can be altered here or overridden with the params
# for createbatch.py.
Parameters:
  ServiceName:
    Type: String
    Default: rios
  VCPUS:
    Type: Number
    Default: 1
  MaxMemory:
    Type: Number
    Default: 4000
  MaxVCPUS:
    Type: Number
    Default: 128
  InstanceType:
    Type: String
    Default: optimal  # for x86. Use m7g etc for Gravitron
  
Resources:
  # Create our own vpc for resources so we are separate
  # from whatever else the running account has.
  # And we can easily determine the output names of resources from
  # CloudFormation
  BatchVPC:
    Type: AWS::EC2::VPC
    Properties:
      CidrBlock: 10.0.0.0/16
      # Below needed for Batch it seems
      EnableDnsSupport: 'true'
      EnableDnsHostnames: 'true'
      # make all instances launch on dedicated hardware
      # when this is 'default' instances are shared
      # making compute intensive job performance quite 
      # unpredictible
      InstanceTenancy: 'dedicated'
  # Create a subnet for each availability zone
  BatchSubnet1:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId:
        Ref: BatchVPC
      # yes we do need public ips or NAT
      # See https://repost.aws/knowledge-center/batch-job-stuck-runnable-status
      MapPublicIpOnLaunch: true
      CidrBlock: 10.0.0.0/24
      AvailabilityZone: !Select 
        - 0
        - Fn::GetAZs: !Ref 'AWS::Region'
  BatchSubnet2:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId:
        Ref: BatchVPC
      MapPublicIpOnLaunch: true
      CidrBlock: 10.0.1.0/24
      AvailabilityZone: !Select 
        - 1
        - Fn::GetAZs: !Ref 'AWS::Region'
  BatchSubnet3:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId:
        Ref: BatchVPC
      MapPublicIpOnLaunch: true
      CidrBlock: 10.0.2.0/24
      AvailabilityZone: !Select 
        - 2
        - Fn::GetAZs: !Ref 'AWS::Region'
  # A security group for everything to run as
  BatchSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupName: !Sub '${ServiceName} Security Group'
      GroupDescription: "Security Group for RIOS VPC"
      VpcId: !Ref BatchVPC

  # Allow traffic between the nodes running the jobs
  BatchIngress:
    Type: AWS::EC2::SecurityGroupIngress
    DependsOn: BatchSecurityGroup
    Properties:
      GroupId: !Ref BatchSecurityGroup
      IpProtocol: tcp
      FromPort: 30000
      ToPort: 50000
      SourceSecurityGroupId: !Ref BatchSecurityGroup

  # Jobs must have internet connectivity otherwise they won't run...
  # They need to talk to ECS/CloudWatch. 
  # Create an endpoint for S3 so traffic doesn't need to go through 
  # the internet gateway (and cost)
  RouteTable:
    Type: "AWS::EC2::RouteTable"
    Properties:
      VpcId: !Ref BatchVPC
  InternetGateway:
    Type: "AWS::EC2::InternetGateway"
  VPCGatewayAttachment:
    Type: "AWS::EC2::VPCGatewayAttachment"
    Properties:
      VpcId: !Ref BatchVPC
      InternetGatewayId: !Ref InternetGateway
  InternetRoute:
    Type: "AWS::EC2::Route"
    Properties:
      DestinationCidrBlock: "0.0.0.0/0"
      GatewayId: !Ref InternetGateway
      RouteTableId: !Ref RouteTable

  # Allow S3 traffic to go through an internet gateway
  S3GatewayEndpoint:
    Type: AWS::EC2::VPCEndpoint
    Properties:
      VpcEndpointType: 'Gateway'
      VpcId: !Ref BatchVPC
      ServiceName: !Sub 'com.amazonaws.${AWS::Region}.s3'
      PolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal: '*'
            Action: 
               - 's3:*Object'
               - 's3:ListBucket'
            Resource: 
               - 'arn:aws:s3:::*/*'
               - 'arn:aws:s3:::*'
      RouteTableIds:
        - !Ref RouteTable

  # Associate the route table with each subnet
  Subnet1RouteTableAssociation:
    Type: "AWS::EC2::SubnetRouteTableAssociation"
    Properties:
      RouteTableId: !Ref RouteTable
      SubnetId: !Ref BatchSubnet1
  Subnet2RouteTableAssociation:
    Type: "AWS::EC2::SubnetRouteTableAssociation"
    Properties:
      RouteTableId: !Ref RouteTable
      SubnetId: !Ref BatchSubnet2
  Subnet3RouteTableAssociation:
    Type: "AWS::EC2::SubnetRouteTableAssociation"
    Properties:
      RouteTableId: !Ref RouteTable
      SubnetId: !Ref BatchSubnet3

  # Create an ECR to hold the image that contains RIOS
  # (and any other packages the function needs)
  # Expires older untagged images
  BatchRepository:
    Type: AWS::ECR::Repository
    Properties:
      RepositoryName: !Sub '${ServiceName}ecr'
      LifecyclePolicy:
        LifecyclePolicyText: |
          {
            "rules": [
                {
                    "rulePriority": 1,
                    "description": "Expire images older than 1 day",
                    "selection": {
                        "tagStatus": "untagged",
                        "countType": "sinceImagePushed",
                        "countUnit": "days",
                        "countNumber": 1
                    },
                    "action": {
                        "type": "expire"
                    }
                }
            ]
          }

  # another repo that the user can copy 'main' docker images with their main scripts
  BatchRepositoryMain:
    Type: AWS::ECR::Repository
    Properties:
      RepositoryName: !Sub '${ServiceName}ecrmain'
      LifecyclePolicy:
        LifecyclePolicyText: |
          {
            "rules": [
                {
                    "rulePriority": 1,
                    "description": "Expire images older than 1 day",
                    "selection": {
                        "tagStatus": "untagged",
                        "countType": "sinceImagePushed",
                        "countUnit": "days",
                        "countNumber": 1
                    },
                    "action": {
                        "type": "expire"
                    }
                }
            ]
          }
       
  # Allow jobs to submit other jobs. Handy in case the user
  # runs their own 'main' script in the queue and wants to 
  # fire off sub jobs
  SubmitJobsManagedPolicy:
    Type: AWS::IAM::ManagedPolicy
    Properties:
      Description: Policy for allowing jobs to submit other jobs
      Path: /
      PolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Action: 
            - 'batch:SubmitJob'
          Resource:
            # Sorry can't nail down to particular queue and defn
            # as this creates a circular dependency
            - !Sub 'arn:aws:batch:${AWS::Region}:${AWS::AccountId}:job-queue/*'
            - !Sub 'arn:aws:batch:${AWS::Region}:${AWS::AccountId}:job-definition/*'
      
  # Needed by AWS Batch.
  BatchServiceRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service: batch.amazonaws.com
          Action: sts:AssumeRole
      ManagedPolicyArns:
      - arn:aws:iam::aws:policy/service-role/AWSBatchServiceRole
  IamInstanceProfile:
    Type: AWS::IAM::InstanceProfile
    Properties:
      Roles:
      - Ref: EcsInstanceRole
  # This is the user that the batch workers run as.
  # Ensure we attach all the permissions it will need.
  EcsInstanceRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2008-10-17'
        Statement:
        - Sid: ''
          Effect: Allow
          Principal:
            Service: ec2.amazonaws.com
          Action: sts:AssumeRole
      ManagedPolicyArns:
      - arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceforEC2Role
      - arn:aws:iam::aws:policy/CloudWatchFullAccess
      - arn:aws:iam::aws:policy/AWSCloudFormationReadOnlyAccess
      - arn:aws:iam::aws:policy/AmazonS3FullAccess
      - !Ref SubmitJobsManagedPolicy
  # The worker job. Set the S3 bucket and SQS queues info in the 
  # enironment.
  BatchProcessingJobDefinition:
    Type: AWS::Batch::JobDefinition
    Properties:
      Type: container
      JobDefinitionName: !Sub '${ServiceName}JobDefinition'
      ContainerProperties:
        Image: !Join ['', [!GetAtt BatchRepository.RepositoryUri, ":latest"]]
        Vcpus: !Ref VCPUS
        Memory: !Ref MaxMemory

  # job definition for the 'main' script
  BatchProcessingJobDefinitionMain:
    Type: AWS::Batch::JobDefinition
    Properties:
      Type: container
      JobDefinitionName: !Sub '${ServiceName}JobDefinitionMain'
      ContainerProperties:
        Image: !Join ['', [!GetAtt BatchRepositoryMain.RepositoryUri, ":latest"]]
        Vcpus: !Ref VCPUS
        Memory: !Ref MaxMemory
        Environment:
          - Name: "RIOS_AWSBATCH_STACK"
            Value: !Ref "AWS::StackName"
          - Name: "RIOS_AWSBATCH_REGION"
            Value: !Ref "AWS::Region"


  # Our queue
  BatchProcessingJobQueue:
    Type: AWS::Batch::JobQueue
    Properties:
      JobQueueName: !Sub '${ServiceName}JobQueue'
      Priority: 1
      ComputeEnvironmentOrder:
      - Order: 1
        ComputeEnvironment:
          Ref: ComputeEnvironment
  # Compute Environment - set subnets and security group etc.
  ComputeEnvironment:
    Type: AWS::Batch::ComputeEnvironment
    Properties:
      Type: MANAGED
      ComputeResources:
        Type: EC2
        MinvCpus: 0
        MaxvCpus: !Ref MaxVCPUS
        InstanceTypes:
        - !Ref InstanceType
        Subnets:
        - !Ref BatchSubnet1
        - !Ref BatchSubnet2
        - !Ref BatchSubnet3
        SecurityGroupIds:
        - !Ref BatchSecurityGroup
        InstanceRole:
          Ref: IamInstanceProfile
      ServiceRole:
        Ref: BatchServiceRole
     
# Outputs that the main script can queury to find
# the names and paths of things.
Outputs:
  VPC:
    Value:
      Ref: BatchVPC
  ComputeEnvironmentArn:
    Value:
      Ref: ComputeEnvironment
  BatchProcessingJobQueueArn:
    Value:
      Ref: BatchProcessingJobQueue
  BatchProcessingJobQueueName:
    Value: !Sub '${ServiceName}JobQueue'
  BatchProcessingJobDefinitionArn:
    Value:
      Ref: BatchProcessingJobDefinition
  BatchProcessingJobDefinitionMainArn:
    Value:
      Ref: BatchProcessingJobDefinitionMain
  BatchProcessingJobDefinitionName:
    Value: !Sub '${ServiceName}JobDefinition'
  BatchProcessingJobDefinitionMainName:
    Value: !Sub '${ServiceName}JobDefinitionMain'
  BatchECR:
    Value: !GetAtt BatchRepository.RepositoryUri
  BatchECRMain:
    Value: !GetAtt BatchRepositoryMain.RepositoryUri
  BatchVCPUS:
    Value: !Ref VCPUS
  BatchMaxVCPUS:
    Value: !Ref MaxVCPUS
